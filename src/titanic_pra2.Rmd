---
title: 'Tipología y ciclo de vida de los datos: PEC 2-Limpieza y análisis de los datos'
author: "Autor: Jorge Santos Neila & Javier Cela López"
date: "Mayo 2020"
output:
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_depth: 2
    includes:
      in_header: 75.584-PEC-header.html
  word_document: default
  pdf_document:
    highlight: zenburn
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
<style>
body {text-align: justify}
</style>

******
# PEC 2 - Limpieza y análisis de los datos
******
## Integrantes
En esta segunda práctica de Tipología y ciclo de via de los datos se realiza un caso práctico orientado a aprender a identificar los datos relevantes para un proyecto analítico y uso las herramientas de integración, limpieza, validación
y análisis de las mismas. Esta práctica ha sido realizada en un grupo de dos formado por las siguientes personas:

* [Jorge Santos Neila](https://github.com/JorgeSaNel)
* [Javier Cela López](https://github.com/javcela10)

Asimismo, el código se ha subido al repositorio de [GitHub](https://github.com/JorgeSaNel/PRA-2-Titanic) y es de uso **METER AQUI SI ES CÖDIGO LIBRE Y POR QUÉ**

## Objetivos
Los objetivos que se persiguen mediante el desarrollo de esta actividad práctica son los siguientes:

* Aprender a aplicar los conocimientos adquiridos y su capacidad de resolución de problemas en entornos nuevos o poco conocidos dentro de contextos más amplios o multidisciplinares.
* Saber identificar los datos relevantes y los tratamientos necesarios (integración, limpieza y validación) para llevar a cabo un proyecto analítico.
* Aprender a analizar los datos adecuadamente para abordar la información contenida en los datos.
* Identificar la mejor representación de los resultados para aportar conclusiones sobre el problema planteado en el proceso analítico.
* Actuar con los principios éticos y legales relacionados con la manipulación de datos en función del ámbito de aplicación.
* Desarrollar las habilidades de aprendizaje que permita continuar estudiando de un modo que tendrá que ser en gran medida autodirigido o autónomo.
* Desarrollar la capacidad de búsqueda, gestión y uso de información y recursos en el ámbito de la ciencia de datos.

## Competencias
Por último, las competencias que se buscan desarrollar a lo largo de esta práctica del Máster de Data Science de la UOC son:

* Capacidad de analizar un problema en el nivel de abstracción adecuado a cada situación y aplicar las habilidades y conocimientos adquiridos para abordarlo y resolverlo.
* Capacidad para aplicar las técnicas específicas de tratamiento de datos (integración, transformación, limpieza y validación) para su posterior análisis.

******
## Organización
A lo largo del código se dará respuesta a las preguntas planteadas por el profesor. Se empezará explicando el dataset seleccionado y los motivos de su elección. A continuación se realizará la tarea de integración y selección de los datos de interés a analizar, seguido del trabajo de limpieza de datos y análisis de datos. El grueso del trabajo estará definido bajo estos dos puntos, ya que son los que más trabajo requieren. Se seguirá con una representación visual de los resultados obtenidos y, para terminar, se expondrán las conclusiones que se han llevado a cabo a lo largo de este ejercicio.

******

# Descripción del dataset
El Conjunto de datos, _Dataset_ en inglés, objeto de análisis se ha obtenido a partir del siguiente enlace en [Kaggle](https://www.kaggle.com/c/titanic/data). No obstante, este es un ejercicio propuesto por Kaggle para hacer predicciones y utilizar algoritmos de machine learning, por lo que el dataset estaba dividido en dos con los siguientes ficheros:

* Train.csv, dataset específico para entrenar el modelo de machine learning.
* Test.csv, dataset específico para testear o probar el modelo realizado.

Debido a que en esta práctica no se van a utilizar modelos de predicción, se han unificado ambos ficheros en uno, llamándolo **titanic-dataset.csv**, el cual está constituido por 12 características (columnas) que presentan 1309 personas (filas o registros). Este dataset esta compuesto por las siguientes variables:

* **PassengerId**: Identificador único del pasajero. Tipo de dato: **integer**.
* **Survived**: Código que identifica si la persona sobrevivió al accidente del Titanic en 1912. Los dos valores que admite esta variable son 0, si la persona no sobrevivió, o 1 en caso contrario. El tipo de dato es un **integer**.
* **Pclass**: Código que identifica la clase en la que viajaba el pasajero. Los valores posibles son;'1', correspondiente a primera clase. '2', clase media. '3', clase baja. El tipo de dato es un **varchar**.
* **Name**: Variable descriptiva que indica el nombre de la persona. Esta variable es una cadena de caracteres del tipo **varchar**.
* **Sex**: Código que identifica el sexo de la persona. Los valores posibles son 'male', si la persona era un hombre, o 'female', si la persona era una mujer. El tipo de dato es un **varchar**.
* **Age**: Número que identifica la edad del pasajero. Esta variable es de tipo **numeric**.
* **SibSp**: Número de hermanos o cónyuges que tenía el pasajero a bordo. Esta variable es de tipo **integer**.
* **Parch**: Número de padres o hijos que tenía cada pasajero a bordo. Esta variable es de tipo **varchar**.
* **Ticket**: Identificador del ticket del pasajero. Esta variable es de tipo **varchar**.
* **Fare**: Tarifa del pasajero para subir a bordo. Esta variable, aunque el sentido común indica que debería ser un float, R lo lee como un **varchar**
* **Cabin**: Identificador de la cabina del pasajero. Esta variable es de tipo **varchar**.
* **Embarked**: Código de embarque del pasajero. Esta variable tiene los siguientes tres valores posibles; 'C' = Cherbourg, 'Q' = Queenstown, 'S' = Southampton. La variable es de tipo **varchar**.

Para comprobar que las vairables descritas tienen el formato correcto, se procede a leer el fichero y mostrar, con la función 'str', el tipo de dato de cada variable:

```{r echo=TRUE, message=FALSE, warning=FALSE}

# Cargamos los paquetes R que vamos a usar
library(ggplot2)
library(dplyr)

# Guardamos el conjunto de datos test y train en un único dataset
# titanic <- read.csv('../dataset/titanic-dataset.csv', stringsAsFactors = FALSE)
test <- read.csv('../dataset/test.csv',stringsAsFactors = FALSE)
train <- read.csv('../dataset/train.csv', stringsAsFactors = FALSE)

# Unimos los dos conjuntos de datos en uno solo
titanic <- bind_rows(train, test)

# Verificamos la estructura del conjunto de datos
str(titanic)

```

## Importancia y objetivos de los análisis
Tras este primer análisis del dataset, la siguiente pregunta que debemos hacernos es por qué es importante este conjunto de datos, y qué problema pretende responder. 

A partir de este conjunto de datos se plantea la problemática de determinar qué variables influyen más sobre la probabilidad de sobrevivir en un accidente como el Titanic, como pueden ser las variables sexo, clase o edad. Además, se podrá proceder a crear modelos de regresión que permitan predecir si un pasajero ha sobrevivido o no en función de sus características y contrastes de hipótesis. Esto ayudará a identificar propiedades interesantes en las muestras que puedan ser inferidas con respecto a la población.

Estos análisis adquieren una gran relevancia ya que nos permite conocer cómo eran las clases sociales de hace 100 años, se puede estudiar su correlación, y evitar así posibles inconsistencias entre clases sociales para los futuros cruceros o eventos, como conseguir que todos tengan la misma probabilidad de sobrevivir independientemente de la situación del pasajero o del dinero que este tenga.

# Integración y selección de los datos
A lo largo de este apartado se dará respuesta a las variables que son interesantes analizar para encontrar la posibilidad de que un pasajero sobreviva al accidente. Podemos dejar fuera todas las variables que, a priori, no tiene relación con la variable objetivo (sobrevivir al accidente), como podrían ser 'Name, 'Cabin' o 'Embarked'. Estas variables, al ser identificadores, no ayudan al modelo. Dicho con otras palabras, no habrá correlación entre estas variables y la posibilidad de sobrevivir. 

Por otro lado, las variables que sí se analizarán son:

* 'Survived' para conocer si el pasajero sobrevivió o no. Esta variable será, en un modelo supervisado, nuestra variable objetivo a predecir.
* A su vez, será interesante conocer si las variables 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch' o 'Fare' tiene correlación con la variable clase. 

Adicionalmente, la variable que queda por comentar es el Identificador único del pasajero, PassengerId'. Esta variable, aunque no va a tener relación con la variable objetivo, está bien dejarla en el análisis ya que es la única que nos permite tener identificado al pasajero.

**NOTA: REVISAR. NO ME GUSTA EL WORDING. El ID tiene sentido dejarlo en un modelo supervisado, pero aquí si no se va a hacer, no sé si tendría sentido**

**NOTA: METER MÄS VARIABLES SI CREEMOS OPORTUNo**

# Limpieza de los datos

Se tratará en este apartado el asunto de la limpieza del conjunto de datos, y nos enfocaremos en dos grandes puntos: la gestión de datos nulos o vacíos (o missing values) y la identificación y tratamiento de valores extremos (o outliers).

## Gestión de datos nulos o vacíos

En primer lugar, identificamos aquellos atributos que tengan valores vacíos. Se seguirá una estrategia individualizada teniendo en cuenta el tipo de atributo del que se trate.

```{r echo=TRUE, message=FALSE, warning=FALSE}

colSums(is.na(titanic))
colSums(titanic=="")

```

Observamos dos tipos de missing values: valores con NA y valores con cadenas vacías. A efectos prácticos, serán tratados de igual forma unos y otros.

Para empezar, como ya comentábamos, la variable Cabin no aporta conocimiento de las personas que sobrevivieron. Es una cadena de caracteres que ni siquiera tiene categorías o valores predeterminados, por lo que no es interesante para el análisis. Eliminamos dicha variable.

```{r echo=TRUE, message=FALSE, warning=FALSE}

titanic$Cabin <- NULL

```

La variable de la edad contiene 263 valores vacíos. En principio, imputar los missing values de esta variable usando la media del resto de edades podría ser una buena estrategia. Observamos la distribución de esta variable para determinar si es así. Si la distribución es normal y centrada en la media, utilizaremos este método. Si la distribución estuviese sesgada hacia algún extremo, encontraremos otra manera.

```{r echo=TRUE, message=FALSE, warning=FALSE}

dens <- density(titanic[!is.na(titanic$Age), "Age"])

n <- length(dens$y)                       #$
dx <- mean(diff(dens$x))                  # Typical spacing in x $
y.unit <- sum(dens$y) * dx                # Check: this should integrate to 1 $
dx <- dx / y.unit                         # Make a minor adjustment
x.mean <- sum(dens$y * dens$x) * dx
y.mean <- dens$y[length(dens$x[dens$x < x.mean])] #$
x.mode <- dens$x[i.mode <- which.max(dens$y)]
y.mode <- dens$y[i.mode]                  #$
y.cs <- cumsum(dens$y)                    #$
x.med <- dens$x[i.med <- length(y.cs[2*y.cs <= y.cs[n]])] #$
y.med <- dens$y[i.med] 

plot(dens, col = "blue", main="Densidad - Age (clase 1)")
mapply(function(x,y,c) lines(c(x,x), c(0,y), lwd=2, col=c), c(x.mean, x.med), c(y.mean, y.med), c("green", "red"))

```

Como se puede apreciar, la función de densidad está ligeramente escorada (o sesgada) hacia la izquierda. La línea roja representa la mediana, y la verde la media. Este escoramiento hace que la media se desplace hacia valores mayores. Esto puede indicar que existen valores extremos o, simplemente, que existen valores altos que encontramos en menor densidad. REcordemos que la media es una medida sensible a este tipo de valores, y por tanto se desplaza. Aunque no es demasiado acusado, quezá sea mejor utilizar la mediana para imputar los valores faltantes.

Dado que se trata de un porcentaje considerable de missing values (263 / 1309), podemos afinar algo más el análisis para imputar los valores de forma más específica. Dividiremos la muestra en las tres clases distintas en las que puede viajar un pasajero. El razonamiento detrás de esto es que es de esperar que la edad sea mayor en las mejores clases (la gente mayor suele viajar con más comodidades que la gente joven). Por tanto, es fácil imaginarse que el atributo de la clase puede tener que ver con la edad del viajero. Vemos a continuación el detalle.

```{r echo=TRUE, message=FALSE, warning=FALSE}

# Clase 1

dens <- density(titanic[!is.na(titanic$Age) & titanic$Pclass == 1, "Age"])

n <- length(dens$y)                       #$
dx <- mean(diff(dens$x))                  # Typical spacing in x $
y.unit <- sum(dens$y) * dx                # Check: this should integrate to 1 $
dx <- dx / y.unit                         # Make a minor adjustment
x.mean <- sum(dens$y * dens$x) * dx
y.mean <- dens$y[length(dens$x[dens$x < x.mean])] #$
x.mode <- dens$x[i.mode <- which.max(dens$y)]
y.mode <- dens$y[i.mode]                  #$
y.cs <- cumsum(dens$y)                    #$
x.med <- dens$x[i.med <- length(y.cs[2*y.cs <= y.cs[n]])] #$
y.med <- dens$y[i.med] 

plot(dens, col = "blue", main="Densidad - Age (clase 1)")
mapply(function(x,y,c) lines(c(x,x), c(0,y), lwd=2, col=c), c(x.mean, x.med), c(y.mean, y.med), c("green", "red"))

# Clase 2

dens <- density(titanic[!is.na(titanic$Age) & titanic$Pclass == 2, "Age"])

n <- length(dens$y)                       #$
dx <- mean(diff(dens$x))                  # Typical spacing in x $
y.unit <- sum(dens$y) * dx                # Check: this should integrate to 1 $
dx <- dx / y.unit                         # Make a minor adjustment
x.mean <- sum(dens$y * dens$x) * dx
y.mean <- dens$y[length(dens$x[dens$x < x.mean])] #$
x.mode <- dens$x[i.mode <- which.max(dens$y)]
y.mode <- dens$y[i.mode]                  #$
y.cs <- cumsum(dens$y)                    #$
x.med <- dens$x[i.med <- length(y.cs[2*y.cs <= y.cs[n]])] #$
y.med <- dens$y[i.med] 

plot(dens, col = "blue", main="Densidad - Age (clase 2)")
mapply(function(x,y,c) lines(c(x,x), c(0,y), lwd=2, col=c), c(x.mean, x.med), c(y.mean, y.med), c("green", "red"))

# Clase 3

dens <- density(titanic[!is.na(titanic$Age) & titanic$Pclass == 3, "Age"])

n <- length(dens$y)                       #$
dx <- mean(diff(dens$x))                  # Typical spacing in x $
y.unit <- sum(dens$y) * dx                # Check: this should integrate to 1 $
dx <- dx / y.unit                         # Make a minor adjustment
x.mean <- sum(dens$y * dens$x) * dx
y.mean <- dens$y[length(dens$x[dens$x < x.mean])] #$
x.mode <- dens$x[i.mode <- which.max(dens$y)]
y.mode <- dens$y[i.mode]                  #$
y.cs <- cumsum(dens$y)                    #$
x.med <- dens$x[i.med <- length(y.cs[2*y.cs <= y.cs[n]])] #$
y.med <- dens$y[i.med] 

plot(dens, col = "blue", main="Densidad - Age (clase 3)")
mapply(function(x,y,c) lines(c(x,x), c(0,y), lwd=2, col=c), c(x.mean, x.med), c(y.mean, y.med), c("green", "red"))

```

Vemos que, para la clase 1, la distribución se centra más en la media, que es cercana a los 40 años. Esto puede ser debido a que no exista mayor concentración de edades bajas que de edades altas como pasa con la muestra total. Es decir, se cumple lo que pronosticábamos: en mejores clases, la edad suele ser más alta. Confirmamos esto viendo que la media y la mediana coinciden prácticamente. 

Para el caso de la clase 2, también encontramos una distribución más centrada. La media se sitúa algo por debajo de los 30 años, y la mediana con un valor ligeramente menor. De nuevo, esta observación confirma la teoría.

Por último, nos encontramos con una densidad para la clase 3 más escorada, y con una media y medianas por debajo de 25 años.

Como sospechábamos, la edad del viajero sí que parece guardar relación con la clase en la que viaje. Utilizaremos las medianas de cada submuestra para imputar los valores nulos de sus observaciones.

```{r echo=TRUE, message=FALSE, warning=FALSE}

titanic[is.na(titanic$Age) & titanic$Pclass == 1, "Age"] = median(titanic[!is.na(titanic$Age) & titanic$Pclass == 1, "Age"])
titanic[is.na(titanic$Age) & titanic$Pclass == 2, "Age"] = median(titanic[!is.na(titanic$Age) & titanic$Pclass == 2, "Age"])
titanic[is.na(titanic$Age) & titanic$Pclass == 3, "Age"] = median(titanic[!is.na(titanic$Age) & titanic$Pclass == 3, "Age"])

```

Encontramos un valor vacío para la variable Fare y dos valores vacíos para la variable Embarked. Al ser números muy reducidos, podemos utilizar la estrategia de imputarlos con la moda (valor que más aparece) para la variable Embarked y la media para la variable Fare sin preocuparnos de introducir sesgos en el conjunto de datos.

```{r echo=TRUE, message=FALSE, warning=FALSE}

titanic[is.na(titanic$Fare), "Fare"] = mean(titanic[!is.na(titanic$Fare), "Fare"])

freq <- table(titanic[!is.na(titanic$Embarked), "Embarked"])
titanic[titanic$Embarked == "", "Embarked"] = freq[which.max(freq)]

```

Para la variable Survived, al ser muy numerosas las observaciones con valores vacíos (lo cual significa que el riesgo de introducir sesgos al imputar mal los datos es grande), utilizaremos un método algo más sofisticado. Seleccionaremos las 10 observaciones más "próximas" y usaremos el valor de dichas observaciones (moda para variables categóricas, media para variables continuas) para imputar nuestros missing values. Este método se conoce como el de los k vecinos más próximos. Usaremos la función kNN del paquete VIM de R con $k = 10$.

```{r echo=TRUE, message=FALSE, warning=FALSE}

library(VIM)

mv <- titanic[, c("Survived", "Pclass", "Sex", "Age", "SibSp", "Parch", "Fare", "Embarked")]

aux_df <- kNN(mv, variable = c("Survived"), k = 10, dist_var = c("Pclass", "Sex", "Age", "SibSp", "Parch", "Fare", "Embarked"))

titanic$Survived <- aux_df$Survived

```

Para terminar, observamos de nuevo la cantidad de valores vacíos. Si todo ha ido bien, ya no deberíamos ver ninguno.

```{r echo=TRUE, message=FALSE, warning=FALSE}

colSums(is.na(titanic))
colSums(titanic=="")

```

## Identificación y tratamiento de valores extremos

# Análisis de los datos
## Selección de los grupos de datos de interés
## Comprobación de la normalidad y homogeneidad de la varianza
## Pruebas estadísticas para comparar los grupos de datos

# Representación gráfica de los resultados

# Conclusiones del análisis

******
# Bibliografía
******

**Dataset**

* [1] Selección del dataset utilizado: https://www.kaggle.com/c/titanic/overview

**Libros**

* [2] Calvo M., Subirats L., Pérez D. (2019). Introducción a la limpieza y análisis de los datos. Editorial UOC.
* [3] Megan Squire (2015). Clean Data. Packt Publishing Ltd.
* [4] Jiawei Han, Micheine Kamber, Jian Pei (2012). Data mining: concepts and techniques. Morgan Kaufmann.
* [5] Jason W. Osborne (2010). Data Cleaning Basics: Best Practices in Dealing with Extreme Scores. Newborn and Infant Nursing Reviews; 10 (1): pp. 1527-3369.
* [6] Peter Dalgaard (2008). Introductory statistics with R. Springer Science & Business Media.
* [7] Wes McKinney (2012). Python for Data Analysis. O’Reilley Media, Inc.